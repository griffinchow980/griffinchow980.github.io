---
title: "项目"
date: 2024-11-20
---

{{< details "**介绍一下你做过的Steam黑神话悟空游戏评论分析项目**" "数据分析" >}}

这是一个基于Steam平台3626条《黑神话·悟空》简体中文评论数据的综合分析项目，主要包含以下几个部分：

**数据采集**：
- 使用Python爬虫（Selenium）从Steam平台爬取游戏评论数据
- 采集了评论内容、推荐状态、游戏时长、发布时间等关键字段
- 处理了动态加载、反爬虫等技术挑战

**数据分析维度**：
- **口碑分析**：推荐率71.15%，正面情感占比58.88%，整体口碑良好
- **用户画像**：分析了玩家游戏时长分布，平均游戏时长54小时
- **内容分析**：通过词频分析和情感分析，发现玩家对剧情、战斗系统、文化元素关注度高
- **时间趋势**：分析评论量随时间的变化趋势

**技术栈**：
- 数据采集：Python + Selenium
- 数据处理：Pandas + NumPy
- 文本分析：jieba分词 + SnowNLP情感分析
- 可视化：ECharts + Matplotlib

**项目价值**：
为游戏运营团队提供了用户口碑洞察，帮助识别玩家关注的核心要素和潜在问题点

{{< /details >}}

{{< details "**电商用户行为分析项目中，你是如何进行用户分群的？**" "用户分析" >}}

这个项目基于1000条电商用户数据，采用RFM模型和K-Means聚类算法进行用户分群：

**RFM模型应用**：
- **R (Recency)**：最近一次购买时间，反映用户活跃度
- **F (Frequency)**：购买频次，反映用户忠诚度  
- **M (Monetary)**：消费金额，反映用户价值

**分群步骤**：

{{< tabs "数据准备,RFM计算,聚类分析,结果应用" >}}

**数据准备阶段**

- 清洗用户行为数据，去除异常值和重复数据
- 统一时间格式和金额单位
- 确保每个用户有完整的交易记录

|||

**RFM指标计算**

```python
# 计算RFM值
current_date = df['date'].max()
rfm = df.groupby('user_id').agg({
    'date': lambda x: (current_date - x.max()).days,  # R
    'order_id': 'count',  # F
    'amount': 'sum'  # M
})
```

- R值：距离最后一次购买的天数（值越小越好）
- F值：总购买次数
- M值：总消费金额

|||

**K-Means聚类**

- 对RFM三个维度进行标准化处理
- 使用肘部法则确定最优聚类数（K=4）
- 应用K-Means算法进行聚类

```python
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# 标准化
scaler = StandardScaler()
rfm_scaled = scaler.fit_transform(rfm)

# K-Means聚类
kmeans = KMeans(n_clusters=4, random_state=42)
rfm['cluster'] = kmeans.fit_predict(rfm_scaled)
```

|||

**业务应用**

根据聚类结果，将用户分为四类：

| 用户类型 | RFM特征 | 用户特点 | 营销策略 | 预期效果 |
|---------|---------|---------|---------|---------|
| **高价值用户** | R高、F高、M高 | 活跃且高价值 | VIP服务、专属权益、优先体验 | 维持忠诚度，提升LTV |
| **潜力用户** | R高、F低、M中 | 新用户或低频用户 | 提升购买频次、会员转化、积分激励 | 转化为高价值用户 |
| **重要挽留用户** | R低、F高、M高 | 流失风险的老客户 | 召回活动、大额优惠券、专人服务 | 重新激活，防止流失 |
| **一般用户** | 中等指标 | 普通用户 | 常规营销、品类推荐、活动推送 | 保持活跃，逐步提升 |

{{< /tabs >}}

**项目成果**：
为不同用户群体制定了差异化营销策略，精准运营提升了整体转化效率

{{< /details >}}

{{< details "**AB测试项目中，如何确保实验的科学性？**" "埋点分析" >}}

在Web新功能点击率AB测试项目中，我遵循了完整的AB测试流程来确保实验科学性：

**实验设计阶段**：

1. **明确假设**：新功能设计能够提升用户点击率
2. **核心指标**：点击率（CTR）作为主要评估指标
3. **样本量计算**：
   - 基于baseline点击率3.5%
   - 期望检测到的最小提升10%
   - 显著性水平α=0.05，统计功效1-β=0.8
   - 计算得每组需要约6万样本

**实验执行阶段**：

{{< admonition type="tip" title="关键控制点" collapse="false" >}}

**随机分流**：
- 采用Hash分流算法确保用户随机分配
- 验证对照组和实验组的用户特征分布一致（年龄、性别、地域等）

**单一变量**：
- 只改变一个变量（新功能设计）
- 其他条件完全一致

**时间控制**：
- 实验周期14天，覆盖两个完整周
- 避免节假日等特殊时期的干扰

**数据埋点**：
- 设计完整的埋点方案，记录曝光、点击、转化等关键事件
- 包含用户ID、实验组别、时间戳等必要字段

{{< /admonition >}}

**数据分析阶段**：

- **假设检验**：使用Mann-Whitney U检验（非参数检验，因为数据不符合正态分布）
- **统计显著性**：p < 0.05，拒绝原假设
- **效应量评估**：实验组点击率3.86% vs 对照组3.47%，提升11.52%
- **置信区间**：计算95%置信区间，确认提升的稳定性

**结果验证**：

- 分时段验证（工作日vs周末）
- 分用户群验证（新用户vs老用户）
- 长期效果追踪（避免新奇效应）

**最终结论**：
实验组显著优于对照组（p<0.05），建议全量上线新功能

{{< /details >}}

{{< details "**岗位数据分析项目中，你发现了哪些有价值的洞察？**" "岗位分析" >}}

这是基于鱼泡直聘284条数据分析岗位数据的分析项目，发现了以下关键洞察：

**薪资水平洞察**：
- 平均薪资14564元/月，中位数13000元/月
- 薪资分布呈右偏态，高薪岗位主要集中在一线城市
- 北上深杭平均薪资17000+，二线城市12000左右

**技能需求洞察**：

{{< tabs "核心技能,进阶技能,加分项" >}}

**必备核心技能（出现频率>80%）**

- SQL：数据查询和处理的基础，几乎所有岗位都要求
- Excel：数据透视、函数应用、图表制作
- Python：数据分析（Pandas、NumPy）、数据可视化

|||

**进阶技能（出现频率50-80%）**

- 数据可视化工具：Tableau、Power BI、ECharts
- 统计分析：假设检验、AB测试、回归分析
- 业务理解：用户分析、漏斗分析、RFM模型

|||

**加分项（出现频率20-50%）**

- 机器学习基础：聚类、分类、预测模型
- 大数据工具：Hive、Spark（大厂要求）
- 爬虫技能：数据采集能力

{{< /tabs >}}

**学历与经验要求**：
- 本科学历占比70%，部分岗位接受优秀专科
- 1-3年经验需求占比最高（55%）
- 应届生岗位占比15%，但要求有项目经验

**地域分布洞察**：
- 一线城市岗位占比45%，新一线城市占比35%
- 深圳、杭州、上海岗位数量位列前三
- 二线城市也有较多机会，但薪资相对较低

**行业分布**：
- 互联网/电商行业需求最大（40%）
- 金融、教育、游戏行业也有较多需求
- 传统行业逐步重视数据分析

{{< admonition type="note" title="求职建议" collapse="false" >}}

基于以上洞察，对求职者的建议：

1. **夯实基础**：SQL、Excel、Python是必备技能，要达到熟练水平
2. **项目经验**：准备2-3个完整的数据分析项目，能够体现分析思路和业务价值
3. **工具拓展**：至少掌握一种BI工具（Tableau/PowerBI）
4. **业务思维**：关注业务场景，不只是技术层面的分析
5. **持续学习**：关注行业动态，了解AB测试、增长分析等进阶方法

{{< /admonition >}}

{{< /details >}}

{{< details "**在项目中遇到数据质量问题，你是如何处理的？**" "数据分析" >}}

数据质量问题是数据分析项目中最常见的挑战，我通常采用以下系统化方法处理：

**问题识别阶段**：

1. **完整性检查**：
   - 检查缺失值比例，超过30%的字段需要评估是否可用
   - 确认关键字段（如user_id、timestamp）的完整性

2. **准确性检查**：
   - 检查数值范围是否合理（如年龄-100到200明显异常）
   - 逻辑一致性验证（如注册时间晚于最后登录时间）

3. **一致性检查**：
   - 同一指标在不同数据源的值是否一致
   - 数据口径是否统一

**处理策略**：

{{< tabs "缺失值,异常值,重复值,格式问题" >}}

**缺失值处理**

根据缺失比例和业务含义选择策略：

- **删除**：缺失比例<5%且为关键字段，直接删除
- **填充均值/中位数**：数值型字段，缺失<20%
- **填充众数**：类别型字段
- **标记为"未知"**：缺失本身有业务含义
- **模型预测**：重要字段可用其他特征预测

```python
# 示例
df['age'].fillna(df['age'].median(), inplace=True)
df['category'].fillna('unknown', inplace=True)
```

|||

**异常值处理**

- **箱线图识别**：IQR方法识别异常值
- **业务判断**：结合业务逻辑判断是否为真实异常
- **处理方式**：
  - 删除：明显错误的数据
  - 截断：将超出范围的值截断到边界
  - 保留：合理的极端值

```python
# 示例：将异常值截断到99分位数
Q99 = df['value'].quantile(0.99)
df.loc[df['value'] > Q99, 'value'] = Q99
```

|||

**重复值处理**

- **完全重复**：保留第一条，删除其他
- **部分重复**：根据业务逻辑判断
  - 如：3秒内的重复点击视为误触，保留一次
  - 如：同一用户同一天的多次购买，全部保留

```python
# 时间窗口去重
df = df.sort_values(['user_id', 'timestamp'])
df['time_diff'] = df.groupby('user_id')['timestamp'].diff()
df = df[(df['time_diff'].isna()) | (df['time_diff'] > 3)]
```

|||

**格式统一**

- **时间格式**：统一转换为标准格式
- **数值精度**：统一小数位数
- **枚举值**：统一大小写、编码
- **单位换算**：统一度量单位

```python
# 时间格式统一
df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')

# 枚举值统一
df['gender'] = df['gender'].str.lower().map({
    'male': 'M', 'female': 'F', 'm': 'M', 'f': 'F'
})
```

{{< /tabs >}}

**实际案例**：

在Steam评论分析项目中，遇到以下数据质量问题：

- **问题1**：约5%的评论缺少游戏时长数据
  - **处理**：这些用户可能是退款或未实际游玩，标记为"未游玩"类别单独分析

- **问题2**：部分评论时长异常（如9999小时）
  - **处理**：验证后发现是挂机用户，保留但在分析时单独标注

- **问题3**：爬取的评论有HTML标签残留
  - **处理**：使用正则表达式清洗，去除所有HTML标签

{{< admonition type="warning" title="数据质量处理原则" collapse="false" >}}

1. **记录所有处理操作**：方便回溯和验证
2. **保留原始数据**：处理后的数据另存，不覆盖原始数据
3. **业务优先**：技术处理要符合业务逻辑
4. **透明化**：在分析报告中说明数据质量情况和处理方式
5. **持续优化**：推动数据埋点和采集的优化

{{< /admonition >}}

{{< /details >}}

{{< details "**如何评估一个数据分析项目的成功？**" "数据分析" >}}

评估数据分析项目的成功需要从多个维度考量，不能仅看分析本身：

**业务价值维度**：

1. **是否解决了业务问题**
   - 项目最初的问题是否得到回答
   - 提供的洞察是否具有可操作性

2. **是否产生了商业影响**
   - 可量化的业务指标提升（如转化率+10%、GMV+100万）
   - 成本节约或效率提升
   - ROI计算：收益/投入

3. **是否影响了决策**
   - 分析结果是否被采纳
   - 是否推动了产品/运营策略的调整

**分析质量维度**：

1. **数据可靠性**
   - 数据来源准确、完整
   - 数据处理过程科学、规范
   - 结论有充分的数据支撑

2. **方法科学性**
   - 使用合适的分析模型和方法
   - 统计检验严谨（如AB测试的显著性）
   - 考虑了混淆因素和偏差

3. **洞察深度**
   - 不止于描述现象，深入挖掘原因
   - 提供了有价值的发现
   - 洞察具有启发性

**交付质量维度**：

1. **沟通效果**
   - 报告清晰易懂，非技术人员也能理解
   - 可视化有效传达了关键信息
   - 建议具体可行

2. **及时性**
   - 按时完成，满足业务需求时间点
   - 关键决策节点前提供分析支持

**实际案例**：

AB测试项目的成功评估：

- ✅ **业务价值**：点击率提升11.52%，预计月GMV增长约50万
- ✅ **分析质量**：12万样本量，统计显著性p<0.05，结论可信
- ✅ **交付质量**：2周完成分析，推动产品全量上线新方案
- ✅ **后续影响**：建立了AB测试流程规范，后续项目可复用

{{< /details >}}

{{< details "**你在项目中用过哪些可视化工具？各有什么优缺点？**" "可视化" >}}

在数据分析项目中，我使用过多种可视化工具，根据不同场景选择：

**Excel图表**：

优点：
- 上手快，学习成本低
- 与数据处理无缝集成
- 适合快速探索和简单报表

缺点：
- 交互性弱，静态图表为主
- 复杂图表制作困难
- 大数据量处理性能差

使用场景：日常数据探索、简单的业务报表

**Tableau**：

优点：
- 可视化效果优秀，专业美观
- 拖拽式操作，无需编程
- 强大的交互功能和Dashboard
- 支持多数据源连接

缺点：
- 商业软件，成本较高
- 复杂定制需要学习
- 性能依赖数据量

使用场景：高管看板、业务分析报告、定期监控报表

**ECharts**：

优点：
- 开源免费，图表类型丰富
- 交互性强，动画效果好
- 可以嵌入网页，适合展示
- 高度可定制

缺点：
- 需要编写代码（JavaScript/JSON）
- 学习曲线较陡
- 需要前端开发知识

使用场景：Web展示、项目文档、技术报告

**Python可视化（Matplotlib/Seaborn）**：

优点：
- 与数据分析流程无缝集成
- 高度灵活，可定制
- 适合科学计算和统计分析

缺点：
- 代码量大，需要编程能力
- 默认样式不够美观（需要调整）
- 交互性弱（静态图为主）

使用场景：数据探索、统计分析、学术研究

**选择建议**：

{{< tabs "快速探索,业务报告,技术展示" >}}

**快速探索阶段**

使用Excel或Python：
- Excel：小数据量，快速查看
- Python：大数据量，复杂分析

|||

**业务报告阶段**

使用Tableau或PowerBI：
- 需要定期更新：Tableau Dashboard
- 需要美观专业：Tableau
- 需要交互探索：Tableau

|||

**技术文档/Web展示**

使用ECharts或Plotly：
- 嵌入网页：ECharts
- 交互探索：Plotly
- 移动端展示：ECharts

{{< /tabs >}}

在我的项目中：
- Steam评论分析：使用ECharts制作词云、情感分布图，嵌入HTML报告
- AB测试项目：使用Tableau制作监控Dashboard，实时追踪指标
- 岗位分析：使用Python+Matplotlib进行探索，ECharts制作最终展示图表

{{< /details >}}

{{< details "**如何从零搭建一个用户画像系统？**" "用户分析" >}}

用户画像是精准营销和个性化推荐的基础，我在电商用户分析项目中实践了完整的构建流程。

**场景**：为电商平台搭建用户画像系统，支持精准营销和个性化推荐。

**完整构建流程**：

**1. 需求分析**

明确画像维度：
- 基础属性：年龄、性别、地域、职业
- 行为属性：浏览频次、购买频次、访问时段
- 偏好属性：品类偏好、价格敏感度、品牌偏好
- 消费属性：客单价、生命周期价值、RFM
- 社交属性：分享行为、评论行为、社交影响力

**2. 标签体系设计**

```python
# 三级标签体系
tags_hierarchy = {
    '用户价值': {
        '高价值用户': ['VIP用户', '超级会员', '企业客户'],
        '中价值用户': ['普通会员', '活跃用户'],
        '低价值用户': ['新用户', '沉睡用户']
    },
    '消费特征': {
        '高频购买': ['每周购买', '月均3单以上'],
        '价格敏感': ['爱用优惠券', '专买促销品']
    }
}

def calculate_user_tags(user_data):
    """计算用户标签"""
    tags = []
    
    # RFM评分
    rfm_score = (user_data['recency_score'] +
                 user_data['frequency_score'] +
                 user_data['monetary_score'])
    
    if rfm_score >= 12:
        tags.append('高价值用户')
    elif rfm_score >= 8:
        tags.append('中价值用户')
    else:
        tags.append('低价值用户')
    
    # 品类偏好
    if user_data['electronics_ratio'] > 0.5:
        tags.append('电子产品爱好者')
    
    # 价格敏感度
    if user_data['avg_discount_usage'] > 0.7:
        tags.append('价格敏感')
    
    return tags
```

**3. 数据ETL流程**

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

def extract_user_data(**context):
    """提取用户数据"""
    date = context['ds']
    
    # 从多个数据源提取
    basic_info = extract_from_crm(date)
    behavior_log = extract_from_clickhouse(date)
    order_data = extract_from_mysql(date)
    
    return {
        'basic': basic_info,
        'behavior': behavior_log,
        'orders': order_data
    }

def transform_user_profile(**context):
    """转换为用户画像"""
    data = context['task_instance'].xcom_pull(task_ids='extract')
    
    profiles = []
    for user_id in data['basic']['user_id'].unique():
        user_behaviors = data['behavior'][
            data['behavior']['user_id'] == user_id
        ]
        user_orders = data['orders'][
            data['orders']['user_id'] == user_id
        ]
        
        profile = {
            'user_id': user_id,
            # 行为特征
            'pv_7d': len(user_behaviors),
            'active_days_30d': user_behaviors['date'].nunique(),
            # 交易特征
            'order_count_30d': len(user_orders),
            'total_amount_30d': user_orders['amount'].sum(),
            # 标签
            'tags': calculate_user_tags(...)
        }
        
        profiles.append(profile)
    
    return profiles

def load_to_database(**context):
    """加载到数据库"""
    profiles = context['task_instance'].xcom_pull(task_ids='transform')
    
    # 存储到MySQL
    pd.DataFrame(profiles).to_sql(
        'user_profiles',
        mysql_engine,
        if_exists='replace'
    )
    
    # 同步到Redis（快速查询）
    for profile in profiles:
        redis_client.hset(
            f"user:{profile['user_id']}",
            mapping=profile
        )

# Airflow DAG
dag = DAG(
    'user_profile_etl',
    default_args={'owner': 'data_team'},
    schedule_interval='@daily',  # 每天执行
    start_date=datetime(2024, 1, 1)
)

extract_task = PythonOperator(
    task_id='extract',
    python_callable=extract_user_data,
    dag=dag
)

transform_task = PythonOperator(
    task_id='transform',
    python_callable=transform_user_profile,
    dag=dag
)

load_task = PythonOperator(
    task_id='load',
    python_callable=load_to_database,
    dag=dag
)

extract_task >> transform_task >> load_task
```

**4. 应用场景**

**场景1：精准营销**
```python
# 筛选目标用户发送优惠券
target_users = session.query(UserProfile).filter(
    UserProfile.tags.contains('高价值用户'),
    UserProfile.tags.contains('电子产品爱好者'),
    UserProfile.recency > 30  # 30天未购买
).all()

for user in target_users:
    send_coupon(user.user_id, category='electronics', discount=0.15)
```

**场景2：个性化推荐**
```python
def recommend_for_user(user_id):
    """基于画像推荐商品"""
    profile = get_user_profile(user_id)
    
    # 根据偏好推荐
    if '电子产品爱好者' in profile.tags:
        products = get_new_electronics()
    
    # 根据价格敏感度
    if '价格敏感' in profile.tags:
        products = filter_by_discount(products)
    
    return products
```

**项目成果**：
- 构建50+用户标签
- 精准营销转化率提升35%
- 推荐点击率提升28%
- 每日自动更新画像

{{< /details >}}

{{< details "**如何设计一次完整的AB测试？**" "AB测试" >}}

AB测试是验证产品优化效果的科学方法，以商品详情页优化为例说明完整流程。

**测试目标**：验证新版详情页能否提升加购转化率。

**关键步骤**：

**1. 假设与指标**

- **假设**：增加用户评价和视频介绍，可提升转化率5%以上
- **核心指标**：加购转化率
- **次要指标**：停留时长、下单率
- **防护指标**：跳出率

**2. 样本量计算**

```python
def calculate_sample_size(baseline=0.15, mde=0.05, alpha=0.05, power=0.8):
    """
    计算所需样本量
    baseline: 基线转化率15%
    mde: 最小可检测效应5%
    alpha: 显著性水平
    power: 统计功效
    """
    import scipy.stats as stats
    import numpy as np
    
    effect_size = mde / np.sqrt(baseline * (1 - baseline))
    z_alpha = stats.norm.ppf(1 - alpha/2)
    z_beta = stats.norm.ppf(power)
    
    n = ((z_alpha + z_beta) / effect_size) ** 2
    
    return int(np.ceil(n))

n_per_group = calculate_sample_size()
print(f"每组需要: {n_per_group}个样本")
# 输出: 每组需要: 6200个样本
```

**3. 流量分配**

```python
import hashlib

def assign_group(user_id, experiment_id):
    """确定性分组"""
    hash_value = int(hashlib.md5(
        f"{user_id}_{experiment_id}".encode()
    ).hexdigest(), 16)
    
    return 'treatment' if hash_value % 2 == 0 else 'control'
```

**4. 埋点实施**

```javascript
// 前端埋点
const group = getExperimentGroup(userId, 'detail_page_v2');

// 页面加载
track('page_view', {
    experiment_id: 'detail_page_v2',
    group: group,
    version: group === 'control' ? 'v1' : 'v2'
});

// 加购行为
document.querySelector('.add-cart').onclick = () => {
    track('add_to_cart', {
        experiment_id: 'detail_page_v2',
        group: group
    });
};
```

**5. 结果分析**

```python
def analyze_ab_test(experiment_id):
    """分析AB测试结果"""
    # 提取数据
    df = get_experiment_data(experiment_id)
    
    control = df[df['group'] == 'control']['converted']
    treatment = df[df['group'] == 'treatment']['converted']
    
    # t检验
    from scipy import stats
    t_stat, p_value = stats.ttest_ind(control, treatment)
    
    # 计算提升
    control_rate = control.mean()
    treatment_rate = treatment.mean()
    lift = (treatment_rate - control_rate) / control_rate
    
    print(f"""
    对照组转化率: {control_rate:.2%}
    实验组转化率: {treatment_rate:.2%}
    相对提升: {lift:.2%}
    P值: {p_value:.4f}
    是否显著: {'是' if p_value < 0.05 else '否'}
    """)
    
    # 决策
    if p_value < 0.05 and lift > 0:
        return "建议全量上线"
    else:
        return "建议继续实验或放弃"
```

**实验结果**：
- 新版转化率提升**6.8%**（p=0.003）
- 停留时长增加15秒
- 跳出率无恶化
- **决策：全量上线**

{{< admonition type="warning" title="常见陷阱" >}}
1. **过早停止**：未达到预定样本量就结束
2. **多重比较**：同时测试多个指标不校正α
3. **新奇效应**：实验时间太短
4. **Simpson悖论**：忽略分层因素
{{< /admonition >}}

{{< /details >}}

{{< details "**遇到数据倾斜问题如何解决？**" "数据处理" >}}

数据倾斜导致Spark任务中个别Task运行时间过长。

**问题表现**：
- 99%的Task在30分钟完成，1%需要4.5小时
- 某Executor内存达95%，其他只有20%
- 频繁OOM错误

**解决方案**：

**方法1：加盐法**

```python
from pyspark.sql.functions import rand, concat, lit

# 原代码（倾斜）
result = orders.groupBy('merchant_id').agg(
    F.sum('amount').alias('total')
)

# 加盐打散
salt_num = 100
orders_salted = orders.withColumn(
    'merchant_id_salted',
    concat(col('merchant_id'), lit('_'), (rand() * salt_num).cast('int'))
)

# 两阶段聚合
partial = orders_salted.groupBy('merchant_id_salted').agg(
    F.sum('amount').alias('partial_sum')
)

final = partial.withColumn(
    'merchant_id',
    F.split(col('merchant_id_salted'), '_')[0]
).groupBy('merchant_id').agg(
    F.sum('partial_sum').alias('total')
)
```

**方法2：分而治之**

```python
# 识别倾斜Key
skewed_threshold = 10000
counts = orders.groupBy('merchant_id').count()
skewed_ids = counts.filter(col('count') > skewed_threshold)

# 分离处理
normal_orders = orders.filter(~col('merchant_id').isin(skewed_ids))
skewed_orders = orders.filter(col('merchant_id').isin(skewed_ids))

# 正常数据正常处理
normal_result = normal_orders.groupBy('merchant_id').agg(...)

# 倾斜数据加盐处理
skewed_result = process_with_salt(skewed_orders)

# 合并
final = normal_result.union(skewed_result)
```

**方法3：广播Join**

```python
# 小表Join大表
from pyspark.sql.functions import broadcast

# 原代码
result = large_orders.join(merchants, 'merchant_id')

# 广播小表
result = large_orders.join(
    broadcast(merchants),  # merchants表小于2GB
    'merchant_id'
)
```

**效果对比**：

| 方法 | 执行时间 | 内存峰值 |
|------|---------|---------|
| 原始 | 5小时 | 95% |
| 加盐法 | 45分钟 | 65% |
| 分而治之 | 35分钟 | 60% |
| 广播Join | 20分钟 | 70% |

{{< /details >}}


